{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/racousin/rag_attack/blob/main/rag_attack.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Initial pour Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/racousin/rag_attack.git\n",
    "%cd rag_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run installation script\n",
    "!bash install_colab.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the rag_attack package\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "‚ö†Ô∏è **Important**: Remplacez les valeurs vides ci-dessous avec vos propres credentials Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - DEMANDEZ VOS CREDENTIALS\n",
    "config = {\n",
    "    'search_endpoint': '',  # Ex: 'https://your-search.search.windows.net'\n",
    "    'search_key': '',  # Votre cl√© Azure Search\n",
    "    'sql_server': '',  # Ex: 'your-server.database.windows.net'\n",
    "    'sql_database': '',  # Nom de votre base de donn√©es\n",
    "    'sql_username': '',  # Username SQL\n",
    "    'sql_password': '',  # Password SQL\n",
    "    'api_base_url': '',  # Ex: 'https://your-api.azurewebsites.net/api'\n",
    "    'openai_endpoint': '',  # Ex: 'https://your-region.api.cognitive.microsoft.com/'\n",
    "    'openai_key': '',  # Votre cl√© OpenAI/Azure OpenAI\n",
    "    'chat_deployment': ''  # Ex: 'gpt-4' ou 'gpt-35-turbo'\n",
    "}\n",
    "\n",
    "# Validate configuration using the rag_attack package\n",
    "from rag_attack import validate_config, test_connection\n",
    "\n",
    "# Test that the package is loaded\n",
    "print(test_connection())\n",
    "\n",
    "# Validate configuration\n",
    "is_valid, message = validate_config(config)\n",
    "if is_valid:\n",
    "    print(\"‚úÖ\", message)\n",
    "else:\n",
    "    print(\"‚ùå\", message)\n",
    "    print(\"\\nVeuillez remplir tous les champs de configuration avant de continuer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1prTZp30Mbfw"
   },
   "source": [
    "# RAG Agentique avec LangGraph - V√©loCorp\n",
    "\n",
    "Ce notebook d√©montre un syst√®me RAG agentique utilisant LangGraph pour orchestrer l'acc√®s √† 3 sources de donn√©es :\n",
    "- üîç **Azure Search** - Documents, FAQ, manuels\n",
    "- üè¢ **Base de donn√©es** - Donn√©es m√©tier (commandes, clients, produits)\n",
    "- üìä **API CRM** - Commerciaux, prospects, opportunit√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3BfbAYv3Mbfz"
   },
   "outputs": [],
   "source": [
    "# N√©cessaire d'effecture ceci avant d'importer Azure Search sinon Azure utilise\n",
    "# sa nomenclature par d√©faut\n",
    "import os\n",
    "os.environ[\"AZURESEARCH_FIELDS_CONTENT_VECTOR\"] = \"embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_VLGispyMbf1"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprebuilt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToolNode\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Dict, Any\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# Imports standards :\n",
    "import functools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Annotated, Dict, Optional, TypedDict\n",
    "\n",
    "# Imports Azure\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "try:\n",
    "    from azure.search.documents.models import VectorizedQuery\n",
    "except ImportError:\n",
    "    VectorizedQuery = None\n",
    "\n",
    "from azure.identity import (\n",
    "    DeviceCodeCredential,\n",
    "    EnvironmentCredential,\n",
    "    ChainedTokenCredential,\n",
    ")\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.mgmt.search import SearchManagementClient\n",
    "from azure.mgmt.sql import SqlManagementClient\n",
    "from azure.mgmt.web import WebSiteManagementClient\n",
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "\n",
    "# Imports third-party\n",
    "import pyodbc\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import AzureSearch\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import (\n",
    "    ChatOpenAI,\n",
    "    AzureChatOpenAI,\n",
    "    AzureOpenAIEmbeddings,\n",
    ")\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import List, Optional, Dict, Any\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K051Nlf29dck",
    "outputId": "3386eeac-c851-4ca5-fdeb-bdcb1e5f5301"
   },
   "outputs": [],
   "source": [
    "# Ce bloc permet d‚Äôinstaller et de configurer le driver ODBC 18 de Microsoft pour se connecter √† SQL Server.\n",
    "\n",
    "# 1. Mise √† jour de la liste des paquets et installation des outils ODBC\n",
    "!apt-get update\n",
    "!apt-get install -y unixodbc unixodbc-dev\n",
    "\n",
    "# 2. Installation du driver MS ODBC 18\n",
    "!curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n",
    "!apt-get install -y curl gnupg\n",
    "!curl https://packages.microsoft.com/config/ubuntu/22.04/prod.list | tee /etc/apt/sources.list.d/mssql-release.list\n",
    "!apt-get update\n",
    "!ACCEPT_EULA=Y apt-get install -y msodbcsql18\n",
    "\n",
    "# 3. V√©rification\n",
    "!odbcinst -j\n",
    "!odbcinst -q -d\n",
    "!odbcinst -q -d -n \"ODBC Driver 18 for SQL Server\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwVVVEMmMbf4"
   },
   "source": [
    "# 1/ RAG simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRAqsaGNMbf5"
   },
   "source": [
    "Etape 1 : charger les manuels utilisateur des v√©los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "E69TPo-Rr47k",
    "outputId": "7d846703-82c5-4cab-b79f-59b64211dabb"
   },
   "outputs": [],
   "source": [
    "# R√©cup√©rer le fichier local\n",
    "uploaded = files.upload()\n",
    "\n",
    "# R√©cup√©rer le nom du fichier upload√© dans colab\n",
    "filename = next(iter(uploaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDE6GI1gMbf6",
    "outputId": "b8d6333c-6446-478d-c8fc-3bf70e1e2da4"
   },
   "outputs": [],
   "source": [
    "def manuals_json_loader(json_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Charge uniquement les documents de type 'manuel_technique' √† partir d'un fichier JSON unifi√©.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Chemin vers le fichier JSON fusionn√© contenant tous les documents.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: Liste de dictionnaires contenant l'id, les m√©tadonn√©es et le texte des manuels techniques.\n",
    "    \"\"\"\n",
    "    # Ouvre le fichier JSON et charge toutes les donn√©es en m√©moire\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    manuals = []  # Liste qui stockera tous les manuels techniques extraits\n",
    "\n",
    "    # Parcourt chaque entr√©e du JSON unifi√©\n",
    "    for key, item in data.items():\n",
    "        meta = item.get(\"metadata\", {})\n",
    "        # On v√©rifie que le document est bien un manuel technique\n",
    "        if meta and meta.get(\"type\", \"\").lower() == \"manuel_technique\":\n",
    "            manuals.append({\n",
    "                \"id\": key,                # identifiant du document (ex: \"manuel_e_sport\")\n",
    "                \"metadata\": meta,         # toutes les m√©tadonn√©es associ√©es\n",
    "                \"text\": item.get(\"text\", \"\")  # le texte int√©gral du manuel\n",
    "            })\n",
    "    return manuals\n",
    "\n",
    "manuals = manuals_json_loader(filename)\n",
    "manuals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Or7q6aglMbf7"
   },
   "outputs": [],
   "source": [
    "def split_by_section(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    D√©coupe un texte en sections √† partir des titres de section en majuscules.\n",
    "    Chaque chunk commence par un titre, suivi de son contenu.\n",
    "\n",
    "    Args:\n",
    "        text (str): Le texte du manuel complet.\n",
    "    Returns:\n",
    "        list of str: Liste des sections trouv√©es dans le texte.\n",
    "    \"\"\"\n",
    "    # Regex pour trouver les titres de section (grandes majuscules sur une ligne)\n",
    "    section_pattern = r\"\\n([A-Z√â√à√Ä√á\\- ]{8,})\\n\"\n",
    "    # On split sur chaque titre d√©tect√©, ce qui donne¬†:\n",
    "    # [introduction, sp√©cification techniques, montage, r√©solution de prob√®mes, ...]\n",
    "    sections = re.split(section_pattern, text)\n",
    "    chunks = []\n",
    "    # sections[0] = introduction √©ventuelle (avant le premier titre)\n",
    "    intro = sections[0].strip()\n",
    "    if intro:\n",
    "        chunks.append(intro)  # On garde l'intro si elle existe\n",
    "    # On parcourt les couples (titre, contenu)\n",
    "    for i in range(1, len(sections), 2): # pas de 2 pour sauter les contenus\n",
    "        title = sections[i].strip()\n",
    "        content = sections[i+1].strip() if i+1 < len(sections) else \"\"\n",
    "        if content:\n",
    "            # On recompose \"titre + contenu\" pour chaque chunk\n",
    "            chunk = f\"{title}\\n{content}\"\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def make_chunk_with_context(doc: Dict[str, Any], section_chunk: str) -> str:\n",
    "    \"\"\"\n",
    "    Ajoute un en-t√™te de contexte (mod√®le, type, ligne de produit) au d√©but de chaque chunk.\n",
    "    Cela permet √† chaque chunk de garder l'identit√© du produit m√™me si le texte ne le mentionne pas.\n",
    "\n",
    "    Args:\n",
    "        doc (dict): Dictionnaire contenant 'metadata' sur le produit.\n",
    "        section_chunk (str): Le texte du chunk (section).\n",
    "    Returns:\n",
    "        str: Le chunk enrichi du contexte produit.\n",
    "    \"\"\"\n",
    "    header = f\"Mod√®le : {doc['metadata'].get('model', '')}\\n\"\n",
    "    header += f\"Type : {doc['metadata'].get('type', '')}\\n\"\n",
    "    header += f\"Ligne de produit : {doc['metadata'].get('product_line', '')}\\n\"\n",
    "    return header + section_chunk\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunking(\n",
    "        manuals: List[Dict[str, Any]],\n",
    "        chunk_size: int = 350,\n",
    "        chunk_overlap: int = 50\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    D√©coupe chaque manuel technique en chunks de taille fixe, avec overlap,\n",
    "    en gardant le contexte du mod√®le dans chaque chunk.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    all_chunks = []\n",
    "    for doc in manuals:\n",
    "        # Nettoyage des s√©parateurs visuels\n",
    "        clean_text = re.sub(r\"(^[-\\s]{3,}$\\n?)+\", \"\", doc[\"text\"], flags=re.MULTILINE)\n",
    "        # D√©coupage global du manuel en chunks overlapping\n",
    "        chunks = splitter.split_text(clean_text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_text = make_chunk_with_context(doc, chunk)\n",
    "            all_chunks.append({\n",
    "                \"doc_id\": doc[\"id\"],\n",
    "                \"chunk_id\": f\"{doc['id']}_chunk_{i}\",\n",
    "                \"metadata\": doc[\"metadata\"],\n",
    "                \"text\": chunk_text\n",
    "            })\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0qf1rmKMbf-"
   },
   "source": [
    "Utilisation du code de chunking et visualisation des r√©sultats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LojJ18exMbf_",
    "outputId": "9d12d9a8-792a-4db9-98c7-bfa56381bae2"
   },
   "outputs": [],
   "source": [
    "all_chunks = chunking(manuals)\n",
    "\n",
    "# Affichage d'exemple\n",
    "for chunk in all_chunks[:2]:\n",
    "    print(\"=\"*100)\n",
    "    print(chunk[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXTGR-VqMbgB"
   },
   "source": [
    "### D√©sormais, nous allons vectoriser (embedding) nos chunks et les stocker pour cr√©er un Vector Store.\n",
    "Un Vector Store est une base de donn√©es sp√©cialis√©e qui stocke des repr√©sentations num√©riques (embeddings) de textes ou documents pour permettre une recherche s√©mantique rapide.\n",
    "Elle permet de retrouver efficacement les passages les plus pertinents par similarit√© de sens, m√™me si les mots exacts ne sont pas pr√©sents dans la requ√™te de l‚Äôutilisateur.\n",
    "\n",
    "FAISS est une biblioth√®que open source permettant la recherche ultra-rapide de similarit√© entre vecteurs dans de grands volumes de donn√©es.\n",
    "On l‚Äôutilise pour retrouver efficacement les passages ou documents les plus pertinents √† partir d‚Äôune requ√™te, m√™me dans des bases contenant des millions de textes ou d‚Äôimages.\n",
    "Parmi les alternatives, on trouve ChromaDB, Qdrant, Weaviate, Milvus, ou encore Annoy et ScaNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcdGgC0DMbgC"
   },
   "outputs": [],
   "source": [
    "def create_faiss_vectorstore(\n",
    "        embedded_chunks: List[Dict[str, Any]],\n",
    "        model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    ) -> FAISS:\n",
    "    \"\"\"\n",
    "    Prend la liste de chunks embedd√©s, construit un vector store FAISS compatible LangChain/LangGraph.\n",
    "    Retourne le vector store pr√™t √† l'emploi.\n",
    "    \"\"\"\n",
    "    # 1. Cr√©ation des objets Document avec contenu et m√©tadonn√©es\n",
    "    docs = [\n",
    "        Document(\n",
    "            page_content=chunk[\"text\"],\n",
    "            metadata=chunk[\"metadata\"]\n",
    "        )\n",
    "        for chunk in embedded_chunks\n",
    "    ]\n",
    "\n",
    "    # 2. Cr√©ation de la fonction d'embedding (wrapper LangChain)\n",
    "    embedding_fn = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    # 3. Cr√©ation du vector store FAISS\n",
    "    vectorstore = FAISS.from_documents(docs, embedding_fn)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmy7NJF1MbgD",
    "outputId": "f3275ec0-63d7-4739-d36f-c8d831eed6cf"
   },
   "outputs": [],
   "source": [
    "# Utilisation de la fonction\n",
    "vectorstore = create_faiss_vectorstore(all_chunks)\n",
    "print(f\"Vector store FAISS cr√©√© avec {vectorstore.index.ntotal} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zF6CTK7MbgD",
    "outputId": "449cdffe-956a-4ff6-e399-2fc7b06a96a9"
   },
   "outputs": [],
   "source": [
    "# Stockage du vectorstore\n",
    "# Chemin de sauvegarde :\n",
    "persist_directory = \"/content/data/faiss_vectorstore\"\n",
    "os.makedirs(persist_directory, exist_ok=True)\n",
    "print(f\"Dossier cr√©√© √† : {persist_directory}\")\n",
    "\n",
    "# Sauvegarde du vector store\n",
    "vectorstore.save_local(persist_directory)\n",
    "print(f\"Vector store FAISS sauvegard√© √† : {persist_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9rpCS8GMbgE"
   },
   "source": [
    "Nous pouvons dor√©navant utiliser notre RAG :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y98hRcUkMbgE"
   },
   "outputs": [],
   "source": [
    "# L'utilisateur saisit sa requ√™te en langage naturel\n",
    "query = \"Quel est le poids du v√©lo Sport-Elite ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55RXIM6PMbgF"
   },
   "source": [
    "Inutile de g√©n√©rer manuellement l'embedding de la requ√™te !\n",
    "La m√©thode 'similarity_search' va automatiquement :\n",
    "   - Passer la requ√™te dans le m√™me mod√®le d'embedding utilis√© pour l'index\n",
    "   - Comparer le vecteur r√©sultant √† tous les embeddings des chunks\n",
    "   - Retourner les k passages les plus proches (selon la similarit√© cosinus)\n",
    "C'est donc un appel \"texte en entr√©e\" ‚ûî \"texte(s) le(s) plus pertinent(s) en sortie\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKJ7EtEgMbgF",
    "outputId": "e37f56e5-e22c-4c30-f928-4f75500cd268"
   },
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- R√©sultat #{i+1} ---\")\n",
    "    print(\"Texte du chunk :\")\n",
    "    print(doc.page_content)\n",
    "    print(\"M√©tadonn√©es :\")\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H00fwYTjMbgG"
   },
   "source": [
    "1/ Instanciation du LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NW1NnhjsMbgG"
   },
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint  = config[\"openai_endpoint\"],\n",
    "    api_key         = config[\"openai_key\"],\n",
    "    deployment_name = config[\"chat_deployment\"],\n",
    "    api_version     = \"2024-02-15-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQvjJQQhMbgI"
   },
   "source": [
    "2/ Exemple de prompt bien formul√© pour que le LLM fasse de la synth√®se RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77CkgDcQMbgI"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Tu es un assistant expert, sp√©cialis√© dans la documentation technique des v√©los.\n",
    "\n",
    "R√©ponds de fa√ßon pr√©cise et concise √† la question utilisateur suivante,\n",
    "en t‚Äôappuyant STRICTEMENT et EXCLUSIVEMENT sur les extraits ci-dessous (issus de la documentation technique officielle)¬†:\n",
    "\\\"\\\"\\\"\n",
    "{context}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "INSTRUCTIONS¬†:\n",
    "- Ne donne aucune information qui ne provient pas explicitement de ces extraits.\n",
    "- Si la r√©ponse exacte (par exemple, une valeur num√©rique, un mod√®le, un tableau) est pr√©sente, cite-la textuellement et indique d'o√π elle provient.\n",
    "- Si plusieurs passages sont n√©cessaires pour une r√©ponse compl√®te, combine-les de fa√ßon claire.\n",
    "- Si l‚Äôinformation demand√©e n‚Äôappara√Æt pas dans les extraits, dis simplement‚ÄØ: ¬´‚ÄØL‚Äôinformation ne figure pas dans la documentation fournie.‚ÄØ¬ª\n",
    "- Reste factuel, sans extrapolation ni interpr√©tation.\n",
    "\n",
    "Question¬†:\n",
    "{query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlrnJXc0MbgH"
   },
   "source": [
    "3/ R√©cup√©rer les chunks les plus pertinents pour la requ√™te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GysmrBkHMbgH"
   },
   "outputs": [],
   "source": [
    "query = \"Quel est le poids v√©lo Urbain-Classic ?\"\n",
    "results = vectorstore.similarity_search(query, k=2)  # on prend les 2 passages les plus pertinents\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98MTn2WgMbgH"
   },
   "source": [
    "4/ Construire le prompt √† envoyer au LLM, en ins√©rant le contexte r√©cup√©r√© et envoyer au LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-B-iF7JMbgK",
    "outputId": "cc0d9480-04b6-447d-96b9-e71ee233da39"
   },
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt.format(context=context, query=query))\n",
    "print(\"R√©ponse du LLM :\\n\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyrC9o03MbgK"
   },
   "source": [
    "6/ comparaison avec une r√©ponse g√©n√©r√©e sans RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayApOK2nMbgL"
   },
   "outputs": [],
   "source": [
    "print(llm.invoke(query).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKvv2oeOYeqs"
   },
   "source": [
    "7/ Test pour une question dont les √©l√©ments de r√©ponse sont absents de la documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-7atFroYqMa"
   },
   "outputs": [],
   "source": [
    "query = \"De quel mat√©riau est fabriqu√© le v√©lo urbain-confort ?\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "response = llm.invoke(prompt.format(context=context, query=query))\n",
    "print(\"R√©ponse du LLM :\\n\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZW33fR9iMbgR"
   },
   "source": [
    "## 2/ Construction et utilisation d'un RAG encapsul√© dans une classe avec Azure\n",
    "#### Ce RAG suit la m√™me logique que pr√©c√©demment, mais en utilisant Azure. Il est encapsul√© dans une classe afin de pouvoir le r√©utiliser comme Tool pour notre RAG agentique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUELizJkMbgR"
   },
   "source": [
    "#### R√©cuperation des outputs terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWOpaB7QMbgT"
   },
   "source": [
    "#### Instanciation des LLMs via Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KObWS5aJMbgT"
   },
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint  = config[\"openai_endpoint\"],\n",
    "    api_key         = config[\"openai_key\"],\n",
    "    deployment_name = config[\"chat_deployment\"],\n",
    "    api_version     = \"2024-02-15-preview\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zAqdPbkMbgT"
   },
   "source": [
    "#### Cr√©ation d'une classe pour r√©cup√©rer uniquement les manuels utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01zytDTSMbgU"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# AzureSearchFilteredRetriever\n",
    "# -----------------------------------------------------------------------------\n",
    "# Petit utilitaire pour interroger directement un index Azure Cognitive Search\n",
    "# en mode \"vector search\" + filtre OData (ex: ne r√©cup√©rer que les documents\n",
    "# dont le champ 'type' vaut 'manuel').\n",
    "#\n",
    "# Pourquoi ce helper ?\n",
    "# - Le wrapper LangChain AzureSearch ne propage pas toujours bien les filtres.\n",
    "# - Ici on utilise le SDK officiel azure-search-documents pour garder le contr√¥le.\n",
    "#\n",
    "# Param√®tres attendus :\n",
    "#   endpoint   : URL du service Azure Search (ex: \"https://xxx.search.windows.net\")\n",
    "#   index_name : nom de l'index (ex: \"documents\")\n",
    "#   key        : cl√© admin ou query key autoris√©e\n",
    "#   embed_fn   : fonction Python qui transforme un texte en vecteur (liste de floats)\n",
    "#\n",
    "# M√©thode principale :\n",
    "#   search_manuals(query, k=5, filter_expr=\"type eq 'manuel'\")\n",
    "#     -> renvoie les r√©sultats bruts (SearchResult-like) retourn√©s par Azure.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class AzureSearchFilteredRetriever:\n",
    "    def __init__(self, endpoint: str, index_name: str, key: str, embed_fn):\n",
    "        # Enregistre les param√®tres de connexion / embedding\n",
    "        self.endpoint = endpoint\n",
    "        self.index_name = index_name\n",
    "        self.embed_fn = embed_fn\n",
    "        # Client natif Azure Search\n",
    "        self.client = SearchClient(endpoint, index_name, AzureKeyCredential(key))\n",
    "\n",
    "    def search_manuals(self, query: str, k: int = 5, filter_expr: str = \"type eq 'manuel'\"):\n",
    "        # 1. embed la requ√™te\n",
    "        vec = self.embed_fn(query)\n",
    "        # 2. construit l‚Äôappel selon SDK\n",
    "        if VectorizedQuery is not None:\n",
    "            # Nouveau SDK\n",
    "            vq = VectorizedQuery(vector=vec, k=k, fields=\"embedding\")\n",
    "            results = self.client.search(\n",
    "                search_text=\"*\",  # pas de fulltext, vector only\n",
    "                vector_queries=[vq],\n",
    "                filter=filter_expr,\n",
    "            )\n",
    "        else:\n",
    "            # Ancien SDK\n",
    "            from azure.search.documents.models import Vector  # fallback\n",
    "            v = Vector(value=vec, k=k, fields=\"embedding\")\n",
    "            results = self.client.search(\n",
    "                search_text=\"*\",\n",
    "                vector=v,\n",
    "                filter=filter_expr,\n",
    "            )\n",
    "\n",
    "        docs = []\n",
    "        for r in results:\n",
    "            # r est un dict-like\n",
    "            docs.append(r)\n",
    "        return docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eHQbtA6MbgV"
   },
   "source": [
    "#### Cr√©ation d'une classe pour le RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psDfNXblMbgn"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configuration logging simple pour afficher les √©tapes du pipeline.\n",
    "# -----------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# RAGPipelineAzure\n",
    "# -----------------------------------------------------------------------------\n",
    "# Ce pipeline combine :\n",
    "#   - un mod√®le d'embedding HuggingFace (utilis√© pour vectoriser les requ√™tes)\n",
    "#   - un LLM h√©berg√© sur Azure OpenAI (chat completions)\n",
    "#   - un index Azure Cognitive Search contenant des documents vectoris√©s\n",
    "#\n",
    "# self.search_client : client natif Azure + retrieve_filtered() -> filtre OData\n",
    "#\n",
    "# Usage typique :\n",
    "#   pipeline = RAGPipelineAzure(...config Terraform...)\n",
    "#   r√©ponse = pipeline.answer(\"Ma question\", k=5)\n",
    "#\n",
    "# NB : l'index contient un champ 'type' (filterable=True) et\n",
    "#       un champ vectoriel 'embedding' (dimensions coh√©rentes avec l'embedding_fn).\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class RAGPipelineAzure:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        api_key: str,\n",
    "        api_base: str,\n",
    "        api_version: str,\n",
    "        deployment_name: str,\n",
    "        config: Dict[str, Any],\n",
    "        index_name: str = \"documents\",\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Initialise la pipeline RAG avec Azure Cognitive Search vectoriel.\n",
    "        \"\"\"\n",
    "        # Sauvegarde du nom du mod√®le d'embedding HuggingFace\n",
    "        self.model_name = model_name\n",
    "        # Initialise la fonction d'embedding (requ√™tes & docs)\n",
    "        self.embedding_fn = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        # Client LLM Azure OpenAI (d√©ploiement chat)\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            azure_endpoint=api_base,\n",
    "            api_key=api_key,\n",
    "            api_version=api_version,\n",
    "            deployment_name=deployment_name,\n",
    "        )\n",
    "        # Nom de l'index Search\n",
    "        self.index_name = index_name\n",
    "\n",
    "        # Client natif Search pour filtrage propre\n",
    "        self.search_client = SearchClient(\n",
    "            config[\"search_endpoint\"],\n",
    "            index_name,\n",
    "            AzureKeyCredential(config[\"search_key\"])\n",
    "        )\n",
    "\n",
    "        # Petite lambda utilitaire : embed une requ√™te texte -> vecteur (liste de floats)\n",
    "        self._embed_query = lambda txt: self.embedding_fn.embed_query(txt)\n",
    "\n",
    "    def retrieve_filtered(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 5,\n",
    "        doc_type: Optional[str] = None\n",
    "    ) -> List[Document]:\n",
    "\n",
    "        \"\"\"Vector search c√¥t√© Azure + filtre OData sur type='...'.\"\"\"\n",
    "        # Embed de la requ√™te utilisateur\n",
    "        vec = self._embed_query(query)\n",
    "        # Expression de filtre OData (doit matcher un champ filterable dans l'index)\n",
    "        filter_expr = f\"type eq '{doc_type}'\" if doc_type else None\n",
    "\n",
    "        results_iter = None\n",
    "        try:\n",
    "            # Tentative nouvelle API\n",
    "            from azure.search.documents.models import VectorizedQuery\n",
    "            vq = VectorizedQuery(vector=vec, k=k, fields=\"embedding\")\n",
    "            results_iter = self.search_client.search(\n",
    "                search_text=\"*\",  # vector-only pattern\n",
    "                vector_queries=[vq],\n",
    "                filter=filter_expr,\n",
    "            )\n",
    "        except ImportError:\n",
    "            # Ancienne API\n",
    "            from azure.search.documents.models import Vector\n",
    "            v = Vector(value=vec, k=k, fields=\"embedding\")\n",
    "            results_iter = self.search_client.search(\n",
    "                search_text=\"*\",\n",
    "                vector=v,\n",
    "                filter=filter_expr,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Toute autre erreur (r√©seau, auth, sch√©ma, etc.)\n",
    "            logger.error(f\"Recherche Azure Search √©chou√©e: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Conversion vers objets LangChain Document pour r√©utiliser la suite du pipeline\n",
    "        from langchain.schema import Document\n",
    "        docs = []\n",
    "        for r in results_iter:\n",
    "            # r est SearchResult -> acc√®s dict-like\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=r.get(\"content\", \"\") if hasattr(r, \"get\") else r[\"content\"],\n",
    "                    metadata={\n",
    "                        \"id\": r[\"id\"] if \"id\" in r else getattr(r, \"id\", None),\n",
    "                        \"filename\": r.get(\"filename\") if hasattr(r, \"get\") else r.get(\"filename\", None) if isinstance(r, dict) else None,\n",
    "                        \"type\": r.get(\"type\") if hasattr(r, \"get\") else r.get(\"type\", None) if isinstance(r, dict) else None,\n",
    "                        \"created\": r.get(\"created\") if hasattr(r, \"get\") else r.get(\"created\", None) if isinstance(r, dict) else None,\n",
    "                        \"document_id\": r.get(\"document_id\") if hasattr(r, \"get\") else r.get(\"document_id\", None) if isinstance(r, dict) else None,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        # On ne retourne que les k premiers (Azure en a d√©j√† retourn√© k, mais on s√©curise)\n",
    "        return docs[:k]\n",
    "\n",
    "    def answer(self, query: str, k: int = 5) -> str:\n",
    "        \"\"\"Renvoie une r√©ponse g√©n√©r√©e par le LLM sur la base des docs filtr√©s.\"\"\"\n",
    "        try:\n",
    "            # R√©cup√©ration des meilleurs chunks filtr√©s c√¥t√© Azure (type='manuel')\n",
    "            results = self.retrieve_filtered(query, k=k, doc_type=\"manuel\")\n",
    "            # Concat√©nation du contenu des chunks pour former le contexte\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "            # Prompt instructif (contexte + question)\n",
    "            prompt = f\"\"\"\n",
    "                Tu es un assistant expert, sp√©cialis√© dans la documentation technique des v√©los.\n",
    "\n",
    "                R√©ponds de fa√ßon pr√©cise et concise √† la question utilisateur suivante,\n",
    "                en t‚Äôappuyant STRICTEMENT et EXCLUSIVEMENT sur les extraits ci-dessous (issus de la documentation technique officielle) :\n",
    "                \\\"\\\"\\\"\n",
    "                {context}\n",
    "                \\\"\\\"\\\"\n",
    "\n",
    "                INSTRUCTIONS :\n",
    "                - Ne donne aucune information qui ne provient pas explicitement de ces extraits.\n",
    "                - Si la r√©ponse exacte (par exemple, une valeur num√©rique, un mod√®le, un tableau) est pr√©sente, cite-la textuellement et indique d'o√π elle provient.\n",
    "                - Si plusieurs passages sont n√©cessaires pour une r√©ponse compl√®te, combine-les de fa√ßon claire.\n",
    "                - Si l‚Äôinformation demand√©e n‚Äôappara√Æt pas dans les extraits, dis simplement : ¬´ L‚Äôinformation ne figure pas dans la documentation fournie. ¬ª\n",
    "                - Reste factuel, sans extrapolation ni interpr√©tation.\n",
    "\n",
    "                Question :\n",
    "                {query}\n",
    "                \"\"\"\n",
    "            # Appel LLM Azure OpenAI\n",
    "            logger.info(\"Envoi du prompt au LLM Azure OpenAI...\")\n",
    "            response = self.llm.invoke(prompt)\n",
    "            logger.info(\"Retour LLM r√©cup√©r√©.\")\n",
    "            content = getattr(response, \"content\", None)\n",
    "            if not content:\n",
    "                logger.warning(\"Pas de contenu retourn√© par le LLM ou r√©ponse vide.\")\n",
    "                return \"Le LLM n'a rien r√©pondu (r√©ponse vide, sans contenu)\"\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la g√©n√©ration de la r√©ponse LLM : {e}\")\n",
    "            return \"Une erreur s'est produite lors de la g√©n√©ration de la r√©ponse.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moowCGQ6Mbgq"
   },
   "outputs": [],
   "source": [
    "pipeline = RAGPipelineAzure(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # Le mod√®le d'embedding HuggingFace utilis√© √† l'indexation\n",
    "    api_key=config[\"openai_key\"],                         # Cl√© API Azure OpenAI sortie du terraform\n",
    "    api_base=config[\"openai_endpoint\"],                   # Endpoint Azure OpenAI sorti du terraform\n",
    "    api_version=\"2024-02-15-preview\",                     # Version API Azure OpenAI (√† ajouter dans ta classe, cf plus bas)\n",
    "    deployment_name=config[\"chat_deployment\"],            # Nom du d√©ploiement Azure OpenAI\n",
    "    config=config,\n",
    "    index_name=\"documents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJehy1YdyN6v"
   },
   "source": [
    "### Tests simple pour v√©rifier le r√©sultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "2r3zrtI7Mbgq",
    "outputId": "34bc6587-163a-4853-e3ec-35169b4bf4ae"
   },
   "outputs": [],
   "source": [
    "# R√©ponse\n",
    "print(pipeline.answer(\"En quelles couleurs est disponible le mod√®le urbain classic ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYTHVIQlMbgr"
   },
   "outputs": [],
   "source": [
    "print(pipeline.answer(\"Quel est le poids du Urbain-Confort ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3GY1GNWMbgr"
   },
   "outputs": [],
   "source": [
    "print(pipeline.answer(\"Que faire si la cha√Æne du v√©lo Pro-Livraison saute ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQYNQFDuyWGj"
   },
   "source": [
    "### Test pour une information absente des manuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDPBj1EFMbgr"
   },
   "outputs": [],
   "source": [
    "print(pipeline.answer(\"Quel est le diam√®tre des roues du v√©lo Pro-Livraison ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0QZQmtDHGyw"
   },
   "source": [
    "# 2/ RAG Agentique simple\n",
    "### -> 2 sources de donn√©es : bases SQL et documentation technique (manuels)\n",
    "- L'orchestrateur choisit quels tools utiliser et renvoie une r√©ponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dr1lfjtqHnhd"
   },
   "outputs": [],
   "source": [
    "# Embedding model (charg√© une seule fois)\n",
    "def _get_embedder(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"Charge une seule fois le mod√®le d'embedding HuggingFace.\"\"\"\n",
    "    global _EMBED_MODEL\n",
    "    if _EMBED_MODEL is None:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        _EMBED_MODEL = SentenceTransformer(model_name)\n",
    "    return _EMBED_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOadJYQeIWbu"
   },
   "source": [
    "Premi√®re √©tape : d√©finir les outils utilisables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4naC0O6HX4e"
   },
   "outputs": [],
   "source": [
    "# DEBUG helpers : simple pr√©fixe pour rep√©rer les traces\n",
    "DBG = lambda msg: print(f\"[DBG] {msg}\")\n",
    "\n",
    "# D√©finition des Tools :\n",
    "\n",
    "# La fonction suivante permet d'aller chercher via recherche s√©mantique un texte r√©pondant √† une question.\n",
    "# Ce code est grandement inspir√© de la classe pr√©c√©dente, mais sans g√©n√©ration de r√©ponse directe\n",
    "@tool\n",
    "def search_documents(query: str, top_results: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Recherche s√©mantique dans l‚Äôindex Azure Cognitive Search **documents**.\n",
    "\n",
    "    ‚öôÔ∏è  Fonctionnement\n",
    "    ------------------\n",
    "    1. Encode la requ√™te utilisateur avec le mod√®le Sentence-Transformers\n",
    "       d√©j√† utilis√© √† l‚Äôindexation (cosine / dot product selon la config).\n",
    "    2. Envoie une requ√™te vectorielle (API `VectorizedQuery` ou `Vector`)\n",
    "       et r√©cup√®re les *k* documents les plus proches.\n",
    "    3. Formate la r√©ponse en Markdown :\n",
    "       **n¬∞. filename** ‚Äì aper√ßu 200 caract√®res ‚Äì *type* ‚Äì *score*.\n",
    "\n",
    "    Param√®tres\n",
    "    ----------\n",
    "    query : str\n",
    "        Texte brut de la question / mot-cl√© √† rechercher.\n",
    "    top_results : int, default = 3\n",
    "        Nombre maximum de documents √† renvoyer.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    str\n",
    "        - Une liste Markdown des r√©sultats.\n",
    "        - ‚ÄúAucun document‚Ä¶‚Äù si rien trouv√©.\n",
    "        - Message d‚Äôerreur clair en cas d‚Äôexception.\n",
    "    \"\"\"\n",
    "\n",
    "    client = SearchClient(config[\"search_endpoint\"], \"documents\",\n",
    "                          AzureKeyCredential(config[\"search_key\"]))\n",
    "    vec = _EMBED_MODEL.encode(query).tolist()\n",
    "\n",
    "    # Choix API Vector / VectorizedQuery selon SDK\n",
    "    try:\n",
    "        from azure.search.documents.models import VectorizedQuery\n",
    "        res = client.search(search_text=\"*\",\n",
    "                            vector_queries=[VectorizedQuery(vector=vec,\n",
    "                                                            k=top_results,\n",
    "                                                            fields=\"embedding\")])\n",
    "    except ImportError:\n",
    "        from azure.search.documents.models import Vector\n",
    "        res = client.search(search_text=\"*\",\n",
    "                            vector=Vector(value=vec, k=top_results,\n",
    "                                          fields=\"embedding\"))\n",
    "    docs = list(res)\n",
    "    if not docs:\n",
    "        return f\"Aucun document pour ¬´ {query} ¬ª.\"\n",
    "\n",
    "    out: List[str] = []\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        snippet = (d.get(\"content\", \"\")[:200] + \"‚Ä¶\").replace(\"\\n\", \" \")\n",
    "        out.append(f\"**{i}. {d.get('filename','?')}** ‚Äì {snippet}\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Ce tool permet d'explorer les tables SQL\n",
    "@tool\n",
    "def explore_database_schema(table_name: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Explore la base SQL Server V√©loCorp :\n",
    "\n",
    "    - Sans argument ‚Üí liste toutes les tables.\n",
    "    - Avec `table_name` ‚Üí d√©tail des colonnes de cette table.\n",
    "\n",
    "    Param√®tres\n",
    "    ----------\n",
    "    table_name : str | None\n",
    "        Nom exact de la table (sensible √† la casse selon la collation).\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    str\n",
    "        - Markdown listant les tables OU\n",
    "        - Tableau Markdown ¬´ colonne / type ¬ª pour la table cibl√©e\n",
    "        - Message d‚Äôerreur lisible si la table n‚Äôexiste pas.\n",
    "\n",
    "    Bonnes pratiques\n",
    "    ----------------\n",
    "    - Appeler cette fonction **avant** de composer une requ√™te SQL complexe.\n",
    "    - Coupler avec `query_database` pour inspecter un √©chantillon.\n",
    "    \"\"\"\n",
    "    conn = (f\"DRIVER={{ODBC Driver 18 for SQL Server}};\"\n",
    "            f\"SERVER={config['sql_server']};DATABASE={config['sql_database']};\"\n",
    "            f\"UID={config['sql_username']};PWD={config['sql_password']};\"\n",
    "            \"Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\")\n",
    "    with pyodbc.connect(conn) as cnx:\n",
    "        cur = cnx.cursor()\n",
    "        if table_name is None:\n",
    "            cur.execute(\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES \"\n",
    "                        \"WHERE TABLE_TYPE='BASE TABLE' ORDER BY 1\")\n",
    "            tables = \"\\n\".join(\"- \" + r[0] for r in cur.fetchall())\n",
    "            DBG(\"Liste des tables SQL :\\n\" + tables)\n",
    "            return tables\n",
    "\n",
    "        cur.execute(\"SELECT COLUMN_NAME, DATA_TYPE \"\n",
    "                    \"FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME=?\",\n",
    "                    table_name)\n",
    "        cols = cur.fetchall()\n",
    "        if not cols:\n",
    "            DBG(f\"Table introuvable : {table_name}\")\n",
    "            return f\"Table ¬´ {table_name} ¬ª introuvable.\"\n",
    "        detail = f\"**{table_name}**\\n\" + \"\\n\".join(f\"- {c[0]} ({c[1]})\"\n",
    "                                                   for c in cols)\n",
    "        DBG(f\"Sch√©ma de {table_name} :\\n\" + detail)\n",
    "        return detail\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Ce Tool permet de faire une requ√™te au serveur SQL\n",
    "@tool\n",
    "def query_database(sql_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Ex√©cute une requ√™te SELECT uniquement sur SQL Server.\n",
    "\n",
    "    S√©curit√© : toute commande non-`SELECT` est rejet√©e imm√©diatement.\n",
    "\n",
    "    Param√®tres\n",
    "    ----------\n",
    "    sql_query : str\n",
    "        Requ√™te SQL compl√®te (peut inclure JOIN, CTE, OFFSET/FETCH, etc.).\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    str\n",
    "        - Tableau Markdown (max 10 lignes)\n",
    "        - ‚ÄúAucun r√©sultat.‚Äù si la s√©lection est vide\n",
    "        - Message d‚Äôerreur enrichi (syntaxe, table inconnue, permissions‚Ä¶).\n",
    "\n",
    "    Exemple\n",
    "    -------\n",
    "    ```python\n",
    "    query_database(\n",
    "        \\\"\\\"\\\"SELECT TOP 5 customer_id, SUM(total) AS CA\n",
    "            FROM invoices GROUP BY customer_id ORDER BY CA DESC\\\"\\\"\\\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # 1) S√©curit√© basique : on bloque tout ce qui n‚Äôest pas SELECT\n",
    "    if not sql_query.strip().upper().startswith(\"SELECT\"):\n",
    "        print(\"[DBG] Blocage s√©curit√© : non-SELECT\")\n",
    "        return \"Seules les requ√™tes SELECT sont autoris√©es.\"\n",
    "\n",
    "    # 2) Connexion SQL Server\n",
    "    conn_str = (f\"DRIVER={{ODBC Driver 18 for SQL Server}};\"\n",
    "                f\"SERVER={config['sql_server']};DATABASE={config['sql_database']};\"\n",
    "                f\"UID={config['sql_username']};PWD={config['sql_password']};\"\n",
    "                \"Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\")\n",
    "\n",
    "    print(\"[DBG] SQL envoy√© :\")\n",
    "    print(sql_query)\n",
    "\n",
    "    with pyodbc.connect(conn_str) as conn:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(sql_query)\n",
    "\n",
    "        cols = [d[0] for d in cur.description]\n",
    "        rows = cur.fetchmany(max_rows)        # ‚ö†Ô∏è on charge max_rows, pas tout\n",
    "\n",
    "    # 3) Debug : combien de lignes ont √©t√© renvoy√©es ?\n",
    "    print(f\"[DBG] Lignes ramen√©es : {len(rows)} / limite affichage {max_rows}\")\n",
    "\n",
    "    if not rows:\n",
    "        print(\"[DBG] Colonnes re√ßues :\", cols)\n",
    "        return \"Aucun r√©sultat.\"\n",
    "\n",
    "    # 4) Formatage Markdown (identique √† ta version)\n",
    "    header = \" | \".join(cols)\n",
    "    sep    = \"-\" * len(header)\n",
    "    body   = [\" | \".join(str(c) for c in r) for r in rows]\n",
    "    return \"```\\n\" + \"\\n\".join([header, sep, *body]) + \"\\n```\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKp8rMQzIa44"
   },
   "source": [
    "Deuxi√®me √©tape : contruiure l'agent LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHizjW1SIBSL"
   },
   "outputs": [],
   "source": [
    "# Construction de l'agent LangGraph\n",
    "\n",
    "# LLM (Azure OpenAI ou OpenAI ‚Äúclassique‚Äù)\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint  = config[\"openai_endpoint\"],\n",
    "    api_key         = config[\"openai_key\"],\n",
    "    deployment_name = config[\"chat_deployment\"],\n",
    "    api_version     = \"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "# Bind des outils ‚Üí le LLM g√©n√©rera directement les appels\n",
    "llm_tools = llm.bind_tools([search_documents,\n",
    "                            explore_database_schema,\n",
    "                            query_database])\n",
    "\n",
    "# √âtat pour LangGraph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "def agent_node(state: AgentState):\n",
    "    system = \"\"\"\n",
    "    Tu es **V√©loCorpGPT**, assistant technique & data.\n",
    "\n",
    "    Outils disponibles\n",
    "    ------------------\n",
    "    1. `search_documents` : recherche s√©mantique dans la doc interne (manuel, FAQ‚Ä¶).\n",
    "    2. `explore_database_schema` : inspecte la structure SQL (tables / colonnes).\n",
    "    3. `query_database` : ex√©cute des requ√™tes SELECT (10 lignes max).\n",
    "\n",
    "    Strat√©gie recommand√©e\n",
    "    ---------------------\n",
    "    - Pour des questions m√©tier (ventes, clients, stock) :\n",
    "      1. Commence par `explore_database_schema()` si tu n‚Äôes pas certain de la table.\n",
    "      2. R√©dige UNE requ√™te SQL compl√®te, puis `query_database()`.\n",
    "      3. Reformule la r√©ponse pour l‚Äôutilisateur (unit√©s, sommes, ordres de grandeur).\n",
    "\n",
    "    - Pour de la documentation technique ou produit :\n",
    "      1. `search_documents()` avec des mots-cl√©s pr√©cis.\n",
    "      2. Synth√©tise la r√©ponse √† partir des extraits retourn√©s.\n",
    "\n",
    "    R√®gles\n",
    "    ------\n",
    "    - Utilise autant d‚Äôoutils que n√©cessaire avant de r√©pondre.\n",
    "    - Ne devine jamais un sch√©ma SQL : v√©rifie-le.\n",
    "    - Garde un style concis et structur√© (titres `###`, listes √† puces, tableaux).\n",
    "\n",
    "    Autres r√®gles relatives aux requ√™tes SQL:\n",
    "    1. **Un SEUL SQL principal** pour r√©pondre : construis une requ√™te agr√©g√©e compl√®te (JOIN/CTE si besoin) au lieu de plusieurs petites requ√™tes ind√©pendantes.\n",
    "    2. **Filtre de dates** : utilise des intervalles (`date >= 'YYYY-01-01' AND date < 'YYYY-02-01'`) au lieu de `MONTH()`/`YEAR()` (meilleure perf + index friendly).\n",
    "    3. **Auto-contr√¥le apr√®s ex√©cution** :\n",
    "      - Si un champ agr√©g√© ressort `NULL`, REFORMULE/REJOINS pour renvoyer 0 au lieu de NULL.\n",
    "      - Si le r√©sultat para√Æt incoh√©rent (ex: 10 commandes mais total NULL), relance une requ√™te corrig√©e.\n",
    "    4. **Toujours afficher la requ√™te ex√©cut√©e** et le sens de chaque colonne retourn√©e dans la r√©ponse finale.\n",
    "    5. **Agr√©gations s√ªres** : enveloppe toute `SUM(...)` ou `COUNT(...)` susceptibles d‚Äô√™tre vides dans `COALESCE(...,0)` pour √©viter `NULL`.\n",
    "    ¬ß. SURTOUT ne devine jamais une table ou une colonne: v√©rifie\n",
    "    \"\"\"\n",
    "    msgs   = [HumanMessage(content=system)] + state[\"messages\"]\n",
    "    resp   = llm_tools.invoke(msgs)\n",
    "    return {\"messages\": state[\"messages\"] + [resp]}\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"\n",
    "    D√©cide si le workflow LangGraph doit :\n",
    "\n",
    "    - ex√©cuter les appels d‚Äôoutils renvoy√©s par le LLM (‚Äútools‚Äù)\n",
    "    - ou s‚Äôarr√™ter (END) et retourner la r√©ponse finale √† l‚Äôutilisateur.\n",
    "\n",
    "    R√®gle :\n",
    "    --------\n",
    "    Si le dernier message contient la cl√© `tool_calls` (c.-√†-d. que le LLM\n",
    "    a demand√© un ou plusieurs outils), on branche vers le n≈ìud ¬´ tools ¬ª.\n",
    "    Sinon, on consid√®re que la r√©ponse est compl√®te et on termine.\n",
    "    \"\"\"\n",
    "    last = state[\"messages\"][-1]\n",
    "    if getattr(last, \"tool_calls\", None):\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.add_node(\"tools\", ToolNode([search_documents,\n",
    "                                  explore_database_schema,\n",
    "                                  query_database]))\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.add_conditional_edges(\"agent\", should_continue,\n",
    "                            {\"tools\": \"tools\", END: END})\n",
    "graph.add_edge(\"tools\", \"agent\")\n",
    "rag_agent = graph.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUxoXwENIjcV"
   },
   "source": [
    "Derni√®re √©tape : cr√©ation d'une fonction pour dialoguer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJ_u9jw-InoO"
   },
   "outputs": [],
   "source": [
    "def ask(question: str):\n",
    "    out = rag_agent.invoke({\"messages\": [HumanMessage(content=question)]})\n",
    "    return out[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvaHsJ3IIqPM"
   },
   "source": [
    "Exemples d'utilisations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "QsOTKXX8IsLH",
    "outputId": "bf0f0bb1-4452-4ca9-8eae-d0ba7cd18315"
   },
   "outputs": [],
   "source": [
    "# Exemple 1: Requ√™te dans la base de donn√©es\n",
    "ask(\"Combien de commandes avons-nous eu au mois de juillet et quelle est leur valeur totale?\")\n",
    "\n",
    "# Point √† noter: il arrive que parfois l'agent n'arrive pas √† calculer le montant et renvoie z√©ro euros\n",
    "# N'h√©sitez pas √† relancer pour obtenir le r√©sultat.\n",
    "# On touche ici aux limites d'un agent simple avec des tools limit√©s\n",
    "# => La requ√™te SQL formul√©e par le LLM n'est pas toujours correcte.\n",
    "# Typiquement pour cet exemple un workflow serait plus adapt√©\n",
    "# (la logique de sortir dans un premier temps le sch√©ma et de b√¢tir la requ√™te √©tant syst√©matique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_ij2_kNIr92"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cS-OfsufMbgr"
   },
   "source": [
    "## üõ†Ô∏è Tool tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YN4KWub6Mbgr"
   },
   "outputs": [],
   "source": [
    "# Classe pour tracker l'usage des outils\n",
    "class ToolUsageTracker:\n",
    "    def __init__(self):\n",
    "        self.tool_calls = []\n",
    "        self.session_start = datetime.now()\n",
    "\n",
    "    def track_tool_call(self, tool_name: str, args: dict, start_time: float, end_time: float,\n",
    "                       success: bool, result_summary: str = \"\", error: str = \"\"):\n",
    "        \"\"\"Enregistre l'utilisation d'un outil\"\"\"\n",
    "        self.tool_calls.append({\n",
    "            \"tool_name\": tool_name,\n",
    "            \"args\": args,\n",
    "            \"start_time\": datetime.fromtimestamp(start_time),\n",
    "            \"end_time\": datetime.fromtimestamp(end_time),\n",
    "            \"duration_ms\": round((end_time - start_time) * 1000, 2),\n",
    "            \"success\": success,\n",
    "            \"result_summary\": result_summary,\n",
    "            \"error\": error\n",
    "        })\n",
    "\n",
    "    def get_usage_summary(self):\n",
    "        \"\"\"Retourne un r√©sum√© de l'usage des outils\"\"\"\n",
    "        if not self.tool_calls:\n",
    "            return \"Aucun outil utilis√©\"\n",
    "\n",
    "        summary = {\n",
    "            \"total_calls\": len(self.tool_calls),\n",
    "            \"total_duration_ms\": sum(call[\"duration_ms\"] for call in self.tool_calls),\n",
    "            \"success_rate\": len([c for c in self.tool_calls if c[\"success\"]]) / len(self.tool_calls) * 100,\n",
    "            \"tools_used\": list(set(call[\"tool_name\"] for call in self.tool_calls)),\n",
    "            \"calls_by_tool\": {}\n",
    "        }\n",
    "\n",
    "        for call in self.tool_calls:\n",
    "            tool = call[\"tool_name\"]\n",
    "            if tool not in summary[\"calls_by_tool\"]:\n",
    "                summary[\"calls_by_tool\"][tool] = {\n",
    "                    \"count\": 0,\n",
    "                    \"total_duration_ms\": 0,\n",
    "                    \"success_count\": 0\n",
    "                }\n",
    "\n",
    "            summary[\"calls_by_tool\"][tool][\"count\"] += 1\n",
    "            summary[\"calls_by_tool\"][tool][\"total_duration_ms\"] += call[\"duration_ms\"]\n",
    "            if call[\"success\"]:\n",
    "                summary[\"calls_by_tool\"][tool][\"success_count\"] += 1\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def get_detailed_log(self):\n",
    "        \"\"\"Retourne le log d√©taill√© des appels d'outils\"\"\"\n",
    "        return self.tool_calls\n",
    "\n",
    "# Instance globale du tracker\n",
    "tool_tracker = ToolUsageTracker()\n",
    "\n",
    "# Wrapper pour tracker les outils automatiquement\n",
    "def tracked_tool(func):\n",
    "    \"\"\"D√©corateur pour tracker automatiquement l'usage des outils\"\"\"\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        tool_name = func.__name__\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            # R√©sum√© du r√©sultat (premiers 100 caract√®res)\n",
    "            result_summary = str(result)[:100] + \"...\" if len(str(result)) > 100 else str(result)\n",
    "\n",
    "            tool_tracker.track_tool_call(\n",
    "                tool_name=tool_name,\n",
    "                args={\"args\": args, \"kwargs\": kwargs},\n",
    "                start_time=start_time,\n",
    "                end_time=end_time,\n",
    "                success=True,\n",
    "                result_summary=result_summary\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "\n",
    "            tool_tracker.track_tool_call(\n",
    "                tool_name=tool_name,\n",
    "                args={\"args\": args, \"kwargs\": kwargs},\n",
    "                start_time=start_time,\n",
    "                end_time=end_time,\n",
    "                success=False,\n",
    "                error=str(e)\n",
    "            )\n",
    "\n",
    "            raise e\n",
    "\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvV89CxzMbgt"
   },
   "source": [
    "## üîß Configuration des Sources de Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3Km73QKMbgt"
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    # Nouvelle API vectorielle\n",
    "    from azure.search.documents.models import VectorizedQuery\n",
    "    _HAS_VQ = True\n",
    "except ImportError:\n",
    "    _HAS_VQ = False\n",
    "\n",
    "# Cache global de l'embedder HuggingFace pour √©viter de recharger √† chaque appel\n",
    "_EMBED_MODEL = None\n",
    "\n",
    "def _get_embedder(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"Charge une seule fois le mod√®le d'embedding HuggingFace.\"\"\"\n",
    "    global _EMBED_MODEL\n",
    "    if _EMBED_MODEL is None:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        _EMBED_MODEL = SentenceTransformer(model_name)\n",
    "    return _EMBED_MODEL\n",
    "\n",
    "# Outils avec tracking automatique\n",
    "@tool\n",
    "@tracked_tool\n",
    "def search_documents(query: str, top_results: int = 3) -> str:\n",
    "    \"\"\"Recherche vectorielle (similarit√© cosinus) dans l'index Azure Search 'documents'.\n",
    "\n",
    "    On encode la requ√™te avec le m√™me mod√®le que celui utilis√© pour indexer les embeddings,\n",
    "    puis on envoie une requ√™te vectorielle √† Azure Search. R√©sultats tri√©s par proximit√©\n",
    "    (cosine / dot, selon la config de l'index HNSW).\n",
    "\n",
    "    Args:\n",
    "        query: Texte de la requ√™te utilisateur.\n",
    "        top_results: Nombre maximum de r√©sultats √† retourner.\n",
    "\n",
    "    Returns:\n",
    "        Cha√Æne format√©e en Markdown listant les meilleurs documents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- Connexion Azure Search --------------------------------------------------\n",
    "        search_credential = AzureKeyCredential(config[\"search_key\"])\n",
    "        search_client = SearchClient(config[\"search_endpoint\"], \"documents\", search_credential)\n",
    "\n",
    "        # --- Embedding de la requ√™te -------------------------------------------------\n",
    "        embedder = _get_embedder()  # charge/cashe le mod√®le HF\n",
    "        query_vec = embedder.encode(query)  # -> numpy array\n",
    "        # Azure attend une liste de floats (pas un np.ndarray)\n",
    "        query_vec = query_vec.tolist()\n",
    "\n",
    "        # --- Construction de la requ√™te vectorielle ---------------------------------\n",
    "        if _HAS_VQ:\n",
    "            # Nouvelle API (>= 11.5 environ)\n",
    "            vq = VectorizedQuery(vector=query_vec, k=top_results, fields=\"embedding\")\n",
    "            results_iter = search_client.search(\n",
    "                search_text=\"*\",               # vector-only pattern\n",
    "                vector_queries=[vq],\n",
    "            )\n",
    "        else:\n",
    "            # Ancienne API (Vector)\n",
    "            from azure.search.documents.models import Vector\n",
    "            v = Vector(value=query_vec, k=top_results, fields=\"embedding\")\n",
    "            results_iter = search_client.search(\n",
    "                search_text=\"*\",\n",
    "                vector=v,\n",
    "            )\n",
    "\n",
    "        # --- Collecte & formatage ----------------------------------------------------\n",
    "        results = list(results_iter)\n",
    "        if not results:\n",
    "            return f\"Aucun document trouv√© (vector search) pour '{query}'.\"\n",
    "\n",
    "        formatted_results = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            # result est SearchResult dict-like\n",
    "            filename = result.get(\"filename\", \"N/A\")\n",
    "            doc_type = result.get(\"type\", \"N/A\")\n",
    "            content_preview = (result.get(\"content\", \"\") or \"\")[:200] + \"...\"\n",
    "            score = result.get(\"@search.score\", 0)\n",
    "\n",
    "            formatted_results.append(\n",
    "                f\"**Document {i}: {filename}** *(type: {doc_type}, score: {score:.4f})*\\n\"\n",
    "                f\"{content_preview}\\n\"\n",
    "            )\n",
    "\n",
    "        return \"\\n\".join(formatted_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Erreur recherche vectorielle documents: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "@tracked_tool\n",
    "def query_database(sql_query: str) -> str:\n",
    "    \"\"\"Ex√©cute une requ√™te SQL sur la base de donn√©es V√©loCorp.\n",
    "\n",
    "    Args:\n",
    "        sql_query: Requ√™te SQL √† ex√©cuter (SELECT uniquement)\n",
    "\n",
    "    Returns:\n",
    "        R√©sultats de la requ√™te format√©s\n",
    "\n",
    "    Note: Si la requ√™te √©choue √† cause d'une table/colonne inconnue,\n",
    "    utilise explore_database_schema() pour voir la structure.\n",
    "    \"\"\"\n",
    "    # V√©rification de s√©curit√© - uniquement SELECT\n",
    "    if not sql_query.strip().upper().startswith('SELECT'):\n",
    "        return \"‚ùå Erreur: Seules les requ√™tes SELECT sont autoris√©es\"\n",
    "\n",
    "    try:\n",
    "        connection_string = f\"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={config['sql_server']};DATABASE={config['sql_database']};UID={config['sql_username']};PWD={config['sql_password']};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    "\n",
    "        with pyodbc.connect(connection_string) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(sql_query)\n",
    "\n",
    "            # R√©cup√©ration des colonnes\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            rows = cursor.fetchall()\n",
    "\n",
    "            if not rows:\n",
    "                return \"‚ÑπÔ∏è Aucun r√©sultat trouv√© pour cette requ√™te\"\n",
    "\n",
    "            # Formatage des r√©sultats\n",
    "            result_lines = [\" | \".join(columns)]\n",
    "            result_lines.append(\"-\" * len(result_lines[0]))\n",
    "\n",
    "            for row in rows[:10]:  # Limite √† 10 r√©sultats\n",
    "                formatted_row = []\n",
    "                for val in row:\n",
    "                    if val is None:\n",
    "                        formatted_row.append(\"NULL\")\n",
    "                    else:\n",
    "                        formatted_row.append(str(val)[:50])  # Limite √† 50 chars\n",
    "                result_lines.append(\" | \".join(formatted_row))\n",
    "\n",
    "            if len(rows) > 10:\n",
    "                result_lines.append(f\"... et {len(rows) - 10} autres r√©sultats\")\n",
    "\n",
    "            # Ajout d'informations contextuelles\n",
    "            result = \"\\n\".join(result_lines)\n",
    "            result += f\"\\n\\n‚úÖ **{len(rows)} r√©sultat(s) trouv√©(s)**\"\n",
    "\n",
    "            return result\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e).lower()\n",
    "\n",
    "        # Messages d'erreur intelligents avec suggestions\n",
    "        if \"invalid object name\" in error_msg or \"invalid column name\" in error_msg:\n",
    "            suggestion = \"\\n\\nüí° **Suggestion**: La table ou colonne semble inexistante. \"\n",
    "            suggestion += \"Utilise explore_database_schema() pour voir les tables disponibles, \"\n",
    "            suggestion += \"ou explore_database_schema('nom_table') pour voir la structure d'une table.\"\n",
    "            return f\"‚ùå Erreur SQL: {str(e)}{suggestion}\"\n",
    "\n",
    "        elif \"syntax error\" in error_msg:\n",
    "            suggestion = \"\\n\\nüí° **Suggestion**: Erreur de syntaxe SQL. \"\n",
    "            suggestion += \"Utilise get_sql_examples() pour voir des exemples de requ√™tes.\"\n",
    "            return f\"‚ùå Erreur de syntaxe: {str(e)}{suggestion}\"\n",
    "\n",
    "        elif \"permission\" in error_msg or \"access\" in error_msg:\n",
    "            return f\"‚ùå Erreur de permissions: {str(e)}\\nSeules les requ√™tes SELECT sont autoris√©es.\"\n",
    "\n",
    "        else:\n",
    "            return f\"‚ùå Erreur base de donn√©es: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "@tracked_tool\n",
    "def query_crm_api(endpoint: str, params: dict = None) -> str:\n",
    "    \"\"\"Interroge l'API CRM V√©loCorp.\n",
    "\n",
    "    Args:\n",
    "        endpoint: Endpoint √† appeler (commerciaux, prospects, opportunites, analytics)\n",
    "        params: Param√®tres optionnels de la requ√™te\n",
    "\n",
    "    Returns:\n",
    "        R√©ponse de l'API format√©e\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"{config['api_base_url']}/crm/{endpoint}\"\n",
    "\n",
    "        response = requests.get(url, params=params or {}, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # Formatage selon l'endpoint\n",
    "        if endpoint == \"commerciaux\":\n",
    "            commerciaux = data.get('commerciaux', [])\n",
    "            result = f\"**{data.get('count', 0)} commerciaux trouv√©s:**\\n\"\n",
    "            for com in commerciaux[:5]:\n",
    "                result += f\"- {com['name']} ({com['email']}) - R√©gions: {', '.join(com['assigned_regions'])}\\n\"\n",
    "            return result\n",
    "\n",
    "        elif endpoint == \"prospects\":\n",
    "            prospects = data.get('prospects', [])\n",
    "            resume = data.get('resume', {})\n",
    "            result = f\"**{data.get('count', 0)} prospects trouv√©s:**\\n\"\n",
    "            result += f\"Score moyen: {resume.get('score_moyen', 0)}\\n\"\n",
    "            for prospect in prospects[:3]:\n",
    "                result += f\"- {prospect['contact_name']} ({prospect['company']}) - Score: {prospect['lead_score']}\\n\"\n",
    "            return result\n",
    "\n",
    "        elif endpoint == \"opportunites\":\n",
    "            opportunites = data.get('opportunites', [])\n",
    "            metriques = data.get('metriques_pipeline', {})\n",
    "            result = f\"**{data.get('count', 0)} opportunit√©s trouv√©es:**\\n\"\n",
    "            result += f\"Valeur pipeline: {metriques.get('valeur_pipeline', 0):,.2f}‚Ç¨\\n\"\n",
    "            for opp in opportunites[:3]:\n",
    "                result += f\"- {opp['title']} - {opp['estimated_value']:,.2f}‚Ç¨ ({opp['status']})\\n\"\n",
    "            return result\n",
    "\n",
    "        elif endpoint == \"analytics\":\n",
    "            globales = data.get('metriques_globales', {})\n",
    "            result = \"**Analytics CRM:**\\n\"\n",
    "            result += f\"Total prospects: {globales.get('total_prospects', 0)}\\n\"\n",
    "            result += f\"Total opportunit√©s: {globales.get('total_opportunites', 0)}\\n\"\n",
    "            result += f\"Valeur pipeline: {globales.get('valeur_pipeline', 0):,.2f}‚Ç¨\\n\"\n",
    "            result += f\"Valeur gagn√©e: {globales.get('valeur_gagnee', 0):,.2f}‚Ç¨\\n\"\n",
    "            return result\n",
    "\n",
    "        else:\n",
    "            return json.dumps(data, indent=2, ensure_ascii=False)[:500] + \"...\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Erreur API CRM: {str(e)}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4XNKfzmMbgw"
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "@tracked_tool\n",
    "def explore_database_schema(table_name: str = None) -> str:\n",
    "    \"\"\"Explore le sch√©ma de la base de donn√©es V√©loCorp.\n",
    "\n",
    "    Args:\n",
    "        table_name: Nom de la table √† explorer (optionnel, si vide retourne toutes les tables)\n",
    "\n",
    "    Returns:\n",
    "        Structure des tables et colonnes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection_string = f\"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={config['sql_server']};DATABASE={config['sql_database']};UID={config['sql_username']};PWD={config['sql_password']};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    "\n",
    "        with pyodbc.connect(connection_string) as conn:\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            if table_name:\n",
    "                # Exploration d'une table sp√©cifique\n",
    "                query = \"\"\"\n",
    "                SELECT\n",
    "                    c.COLUMN_NAME,\n",
    "                    c.DATA_TYPE,\n",
    "                    c.IS_NULLABLE,\n",
    "                    c.COLUMN_DEFAULT,\n",
    "                    CASE WHEN pk.COLUMN_NAME IS NOT NULL THEN 'PK' ELSE '' END as IS_PRIMARY_KEY,\n",
    "                    CASE WHEN fk.COLUMN_NAME IS NOT NULL THEN\n",
    "                        'FK -> ' + fk.REFERENCED_TABLE_NAME + '(' + fk.REFERENCED_COLUMN_NAME + ')'\n",
    "                    ELSE '' END as FOREIGN_KEY\n",
    "                FROM INFORMATION_SCHEMA.COLUMNS c\n",
    "                LEFT JOIN (\n",
    "                    SELECT ku.TABLE_NAME, ku.COLUMN_NAME\n",
    "                    FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS tc\n",
    "                    JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE ku\n",
    "                        ON tc.CONSTRAINT_NAME = ku.CONSTRAINT_NAME\n",
    "                    WHERE tc.CONSTRAINT_TYPE = 'PRIMARY KEY'\n",
    "                ) pk ON c.TABLE_NAME = pk.TABLE_NAME AND c.COLUMN_NAME = pk.COLUMN_NAME\n",
    "                LEFT JOIN (\n",
    "                    SELECT\n",
    "                        ku.TABLE_NAME, ku.COLUMN_NAME,\n",
    "                        ku2.TABLE_NAME as REFERENCED_TABLE_NAME,\n",
    "                        ku2.COLUMN_NAME as REFERENCED_COLUMN_NAME\n",
    "                    FROM INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS rc\n",
    "                    JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE ku\n",
    "                        ON rc.CONSTRAINT_NAME = ku.CONSTRAINT_NAME\n",
    "                    JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE ku2\n",
    "                        ON rc.UNIQUE_CONSTRAINT_NAME = ku2.CONSTRAINT_NAME\n",
    "                ) fk ON c.TABLE_NAME = fk.TABLE_NAME AND c.COLUMN_NAME = fk.COLUMN_NAME\n",
    "                WHERE c.TABLE_NAME = ?\n",
    "                ORDER BY c.ORDINAL_POSITION\n",
    "                \"\"\"\n",
    "                cursor.execute(query, table_name)\n",
    "                columns = cursor.fetchall()\n",
    "\n",
    "                if not columns:\n",
    "                    return f\"Table '{table_name}' non trouv√©e\"\n",
    "\n",
    "                result = f\"**Structure de la table {table_name}:**\\n\"\n",
    "                result += \"| Colonne | Type | Nullable | D√©faut | Cl√© |\\n\"\n",
    "                result += \"|---------|------|----------|--------|-----|\\n\"\n",
    "\n",
    "                for col in columns:\n",
    "                    key_info = f\"{col[4]} {col[5]}\".strip()\n",
    "                    result += f\"| {col[0]} | {col[1]} | {col[2]} | {col[3] or 'NULL'} | {key_info} |\\n\"\n",
    "\n",
    "                # Ajouter un √©chantillon de donn√©es\n",
    "                sample_query = f\"SELECT TOP 3 * FROM {table_name}\"\n",
    "                cursor.execute(sample_query)\n",
    "                sample_data = cursor.fetchall()\n",
    "\n",
    "                if sample_data:\n",
    "                    result += f\"\\n**√âchantillon de donn√©es:**\\n\"\n",
    "                    column_names = [desc[0] for desc in cursor.description]\n",
    "                    result += \" | \".join(column_names) + \"\\n\"\n",
    "                    result += \"-\" * len(\" | \".join(column_names)) + \"\\n\"\n",
    "\n",
    "                    for row in sample_data:\n",
    "                        formatted_row = [str(val) if val is not None else 'NULL' for val in row]\n",
    "                        result += \" | \".join(formatted_row) + \"\\n\"\n",
    "\n",
    "                return result\n",
    "\n",
    "            else:\n",
    "                # Liste de toutes les tables\n",
    "                query = \"\"\"\n",
    "                SELECT\n",
    "                    t.TABLE_NAME,\n",
    "                    COUNT(c.COLUMN_NAME) as COLUMN_COUNT,\n",
    "                    STRING_AGG(c.COLUMN_NAME, ', ') as COLUMNS\n",
    "                FROM INFORMATION_SCHEMA.TABLES t\n",
    "                LEFT JOIN INFORMATION_SCHEMA.COLUMNS c ON t.TABLE_NAME = c.TABLE_NAME\n",
    "                WHERE t.TABLE_TYPE = 'BASE TABLE'\n",
    "                GROUP BY t.TABLE_NAME\n",
    "                ORDER BY t.TABLE_NAME\n",
    "                \"\"\"\n",
    "                cursor.execute(query)\n",
    "                tables = cursor.fetchall()\n",
    "\n",
    "                result = \"**Tables disponibles dans la base V√©loCorp:**\\n\"\n",
    "                for table in tables:\n",
    "                    result += f\"- **{table[0]}** ({table[1]} colonnes)\\n\"\n",
    "                    if table[2]:\n",
    "                        columns_preview = table[2][:100] + \"...\" if len(table[2]) > 100 else table[2]\n",
    "                        result += f\"  Colonnes: {columns_preview}\\n\"\n",
    "\n",
    "                result += \"\\nUtilise explore_database_schema('nom_table') pour plus de d√©tails sur une table sp√©cifique.\"\n",
    "                return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Erreur exploration sch√©ma: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZB_opaQMbgy",
    "outputId": "7e112a43-e1c9-48ee-9239-697fded55670"
   },
   "outputs": [],
   "source": [
    "# Liste des outils disponibles\n",
    "tools = [search_documents, query_database, query_crm_api, explore_database_schema]\n",
    "\n",
    "# LLM avec outils\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# √âtat du graphe\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"N≈ìud principal de l'agent avec instructions d√©taill√©es pour la DB\"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    Tu es un assistant intelligent pour V√©loCorp, entreprise de v√©los.\n",
    "    Tu as acc√®s √† 5 outils sp√©cialis√©s :\n",
    "\n",
    "    ## üîç OUTILS DISPONIBLES:\n",
    "\n",
    "    1. **explore_database_schema()** - Explorer la structure de la DB\n",
    "       - Sans param√®tre : liste toutes les tables\n",
    "       - Avec nom_table : d√©tails d'une table sp√©cifique\n",
    "\n",
    "    2. **get_sql_examples(category)** - Exemples de requ√™tes SQL\n",
    "       - Cat√©gories : clients, commandes, produits, analytics, all\n",
    "\n",
    "    3. **query_database(sql_query)** - Ex√©cuter des requ√™tes SQL\n",
    "       - Uniquement SELECT autoris√©\n",
    "       - Messages d'erreur intelligents avec suggestions\n",
    "\n",
    "    4. **search_documents(query)** - Rechercher dans la documentation\n",
    "       - Manuels, FAQ, emails internes\n",
    "\n",
    "    5. **query_crm_api(endpoint)** - API CRM\n",
    "       - Endpoints : commerciaux, prospects, opportunites, analytics\n",
    "\n",
    "    ##### üéØ STRAT√âGIE OPTIMALE:\n",
    "\n",
    "    ### STRAT√âGIE OUTILS\n",
    "\n",
    "    - √âtape 1 : `explore_database_schema()` (global puis tables candidates).\n",
    "    - √âtape 2 : G√©n√®re la requ√™te SQL compl√®te ‚Üí `query_database()`.\n",
    "    - √âtape 3 : Si r√©sultat incomplet/NULL, corrige et relance (ne PAS r√©pondre tant que les 2 m√©triques demand√©es ne sont pas num√©riques).\n",
    "\n",
    "    Autres r√®gles relatives aux requ√™tes SQL:\n",
    "    1. **Un SEUL SQL principal** pour r√©pondre : construis une requ√™te agr√©g√©e compl√®te (JOIN/CTE si besoin) au lieu de plusieurs petites requ√™tes ind√©pendantes.\n",
    "    2. **Toujours v√©rifier le sch√©ma avant d‚Äô√©crire du SQL** avec `explore_database_schema()` et r√©sumer mentalement:\n",
    "      - quelles tables contiennent les commandes ?\n",
    "      - o√π se trouve le montant (orders.total ? order_items.unit_price*quantity ? invoices.amount ?).\n",
    "    3. **Filtre de dates** : utilise des intervalles (`date >= 'YYYY-01-01' AND date < 'YYYY-02-01'`) au lieu de `MONTH()`/`YEAR()` (meilleure perf + index friendly).\n",
    "    4. **Agr√©gations s√ªres** : enveloppe toute `SUM(...)` ou `COUNT(...)` susceptibles d‚Äô√™tre vides dans `COALESCE(...,0)` pour √©viter `NULL`.\n",
    "    5. **Pr√©server le comptage des commandes m√™me sans facture** : utilise des `LEFT JOIN` depuis `orders` vers les autres tables.\n",
    "    6. **Auto-contr√¥le apr√®s ex√©cution** :\n",
    "      - Si un champ agr√©g√© ressort `NULL`, REFORMULE/REJOINS pour renvoyer 0 au lieu de NULL.\n",
    "      - Si le r√©sultat para√Æt incoh√©rent (ex: 10 commandes mais total NULL), relance une requ√™te corrig√©e.\n",
    "    7. **Toujours afficher la requ√™te ex√©cut√©e** et le sens de chaque colonne retourn√©e dans la r√©ponse finale.\n",
    "    8. **Ne pas confondre ‚Äúvaleur des commandes‚Äù et ‚Äúmontant factur√©‚Äù** :\n",
    "      - si la question dit ‚Äúvaleur des commandes‚Äù, calcule √† partir d‚Äô`orders` ou `order_items`; les factures ne sont qu‚Äôun proxy √©ventuel.\n",
    "\n",
    "    Respecte ces r√®gles avant de r√©pondre.\n",
    "\n",
    "    **Pour les questions sur la base de donn√©es :**\n",
    "    1. üîç **TOUJOURS commencer par explore_database_schema()** si tu ne connais pas la structure exacte\n",
    "    2. üìö Si besoin d'inspiration pour les requ√™tes ‚Üí get_sql_examples()\n",
    "    3. ‚ö° Puis ex√©cuter ‚Üí query_database()\n",
    "\n",
    "    **Exemples de workflow :**\n",
    "    - \"Nos meilleurs clients\" ‚Üí explore_database_schema('clients') ‚Üí query_database(...)\n",
    "    - \"Ventes du mois\" ‚Üí explore_database_schema('commandes') ‚Üí query_database(...)\n",
    "    - \"Stock v√©los\" ‚Üí explore_database_schema('produits') ‚Üí query_database(...)\n",
    "\n",
    "    **Pour autres types de questions :**\n",
    "    - Documentation/FAQ ‚Üí search_documents()\n",
    "    - Performance commerciale ‚Üí query_crm_api()\n",
    "\n",
    "    ## ‚ö° R√àGLES IMPORTANTES:\n",
    "    - Ne devines JAMAIS la structure des tables\n",
    "    - Utilise explore_database_schema() avant toute requ√™te SQL complexe\n",
    "    - Sois pr√©cis et concis dans tes r√©ponses\n",
    "    - Priorise la qualit√© des donn√©es sur la rapidit√©\n",
    "\n",
    "    **Tu es maintenant pr√™t √† aider efficacement avec V√©loCorp !** üö¥‚Äç‚ôÇÔ∏è\n",
    "    \"\"\"\n",
    "\n",
    "    '''\n",
    "    messages = [HumanMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    print(\"Prompt envoy√© au LLM avec outils:\", messages)\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    print(\"r√©ponse brute du LLM\")\n",
    "    return {\"messages\": [response]}\n",
    "    '''\n",
    "    messages = [HumanMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "\n",
    "# Fonction de routage\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"D√©termine si on continue ou on termine\"\"\"\n",
    "    print(\"Check du should_continue, messages actuels :\", state[\"messages\"])\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    print(\"On termine l'agent.\")\n",
    "\n",
    "    return END\n",
    "\n",
    "# Construction du graphe\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Ajout des n≈ìuds\n",
    "print(\"Ajout du noeud agent\")\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "print(\"Ajout du noeud tools\")\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# D√©finition des edges\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compilation du graphe\n",
    "app = workflow.compile()\n",
    "print(\"ü§ñ Agent LangGraph cr√©√© avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0NAOYCXMbgy"
   },
   "source": [
    "## ü§ñ Cr√©ation de l'Agent LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-B-2WQ13Mbgz"
   },
   "source": [
    "## üéØ Fonction d'Interaction Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98VIrpknMbgz"
   },
   "outputs": [],
   "source": [
    "def ask_agent(\n",
    "    question: str,\n",
    "    verbose: bool = True,\n",
    "    show_tool_details: bool = False,\n",
    "    show_usage_stats: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Pose une question √† l'agent et affiche la r√©ponse avec options de diagnostic\n",
    "    \"\"\"\n",
    "    print(\"\\n‚ùì **Question:**\", question)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Reset du tracker pour cette question\n",
    "    global tool_tracker\n",
    "    tool_tracker = ToolUsageTracker()\n",
    "\n",
    "    try:\n",
    "        print(\"[DEBUG] D√©but ex√©cution agent...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ENVOI DE LA QUESTION √Ä L'AGENT\n",
    "        result = app.invoke({\"messages\": [HumanMessage(content=question)]})\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\"[DEBUG] R√©sultat brut de l'agent :\")\n",
    "        print(result)\n",
    "\n",
    "        # TEST CL√â: Y A-T-IL BIEN UN CHAMP 'messages' ?\n",
    "        if \"messages\" not in result:\n",
    "            print(\"[DEBUG] Pas de cl√© 'messages' dans le r√©sultat !\")\n",
    "            return\n",
    "\n",
    "        print(\"[DEBUG] Messages retourn√©s :\")\n",
    "        print(result[\"messages\"])\n",
    "\n",
    "        # On prend le dernier message\n",
    "        last_message = result[\"messages\"][-1]\n",
    "        print(\"[DEBUG] Dernier message :\")\n",
    "        print(last_message)\n",
    "\n",
    "        # REGARDE LE CHAMP content\n",
    "        if hasattr(last_message, 'content'):\n",
    "            final_response = last_message.content\n",
    "        elif isinstance(last_message, dict) and \"content\" in last_message:\n",
    "            final_response = last_message[\"content\"]\n",
    "        else:\n",
    "            print(\"[DEBUG] Aucun champ 'content' trouv√© dans le dernier message.\")\n",
    "            final_response = \"\"\n",
    "\n",
    "        print(f\"ü§ñ **R√©ponse:** {final_response}\")\n",
    "\n",
    "        # Affichage du temps total\n",
    "        total_time = round((end_time - start_time) * 1000, 2)\n",
    "        print(f\"\\n‚è±Ô∏è **Temps total:** {total_time}ms\")\n",
    "\n",
    "        # Reste de la logique (outils/diagnostic)\n",
    "        if verbose or show_tool_details or show_usage_stats:\n",
    "            usage_summary = tool_tracker.get_usage_summary()\n",
    "\n",
    "            if verbose and usage_summary != \"Aucun outil utilis√©\":\n",
    "                print(f\"\\nüîß **Outils utilis√©s:** {', '.join(usage_summary['tools_used'])}\")\n",
    "                print(f\"üìä **Nombre d'appels:** {usage_summary['total_calls']}\")\n",
    "                print(f\"‚úÖ **Taux de succ√®s:** {usage_summary['success_rate']:.1f}%\")\n",
    "\n",
    "            if show_usage_stats and usage_summary != \"Aucun outil utilis√©\":\n",
    "                print(f\"\\nüìà **Statistiques d√©taill√©es:**\")\n",
    "                print(f\"   - Temps total outils: {usage_summary['total_duration_ms']}ms\")\n",
    "\n",
    "                for tool_name, stats in usage_summary['calls_by_tool'].items():\n",
    "                    success_rate = (stats['success_count'] / stats['count']) * 100\n",
    "                    avg_time = stats['total_duration_ms'] / stats['count']\n",
    "                    print(f\"   - {tool_name}: {stats['count']} appels, {avg_time:.1f}ms moyen, {success_rate:.1f}% succ√®s\")\n",
    "\n",
    "            if show_tool_details and usage_summary != \"Aucun outil utilis√©\":\n",
    "                print(f\"\\nüîç **D√©tails des appels d'outils:**\")\n",
    "                for i, call in enumerate(tool_tracker.get_detailed_log(), 1):\n",
    "                    status = \"‚úÖ\" if call['success'] else \"‚ùå\"\n",
    "                    print(f\"   {i}. {status} {call['tool_name']} ({call['duration_ms']}ms)\")\n",
    "                    if call['args']['kwargs']:\n",
    "                        print(f\"      Args: {call['args']['kwargs']}\")\n",
    "                    if not call['success']:\n",
    "                        print(f\"      Erreur: {call['error']}\")\n",
    "                    elif call['result_summary']:\n",
    "                        print(f\"      R√©sultat: {call['result_summary']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå **Erreur:** {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMAviQC2ye89"
   },
   "source": [
    "### Illustration des retours du tool \"explore_database_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dSc1ynCMbg0",
    "outputId": "8d1a1162-f66c-4221-d39d-4fe0c65c9d48"
   },
   "outputs": [],
   "source": [
    "print(explore_database_schema.invoke({}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4V1O9An6Mbg1",
    "outputId": "d07aa50c-8ca9-45dc-cf1c-0d6719dea1cb"
   },
   "outputs": [],
   "source": [
    "print(explore_database_schema('customers'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xR8xwOjlyt3j"
   },
   "source": [
    "### Illustration du retour du Tool \"query_api_crm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDR8v6EhMbg1",
    "outputId": "0b69d067-200a-43ba-ce89-cc1c023aff27"
   },
   "outputs": [],
   "source": [
    "print(query_crm_api(\"commerciaux\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lp60ODRMbg1"
   },
   "source": [
    "# Exemples d'Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "McZLKUa8Mbg2",
    "outputId": "bf73ed8c-2d5e-4db4-9c0f-c8544938fbce"
   },
   "outputs": [],
   "source": [
    "# Exemple 1: Requ√™te dans la base de donn√©es\n",
    "ask_agent(\"Combien de commandes avons-nous eu au mois de janvier et quelle est leur valeur totale?\")\n",
    "\n",
    "# Point √† noter: il arrive que parfois l'agent n'arrive pas √† calculer le montant et renvoie z√©ro euros\n",
    "# N'h√©sitez pas √† relancer pour obtenir le r√©sultat.\n",
    "# On touche ici aux limites d'un agent simple avec des tools limit√©s\n",
    "# => La requ√™te SQL formul√©e par le LLM n'est pas toujours correcte.\n",
    "# Typiquement pour cet exemple un workflow serait plus adapt√©\n",
    "# (la logique de sortir dans un premier temps le sch√©ma et de b√¢tir la requ√™te √©tant syst√©matique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMawniPKMbg2"
   },
   "outputs": [],
   "source": [
    "# Exemple 2 : recherche vectorielle et CRM\n",
    "ask_agent(\n",
    "    question=\"Dans quelles couleurs est disponible le v√©lo urbain-confort et quels sont ses clients potentiels ?\",\n",
    "    verbose=True,\n",
    "    show_tool_details=True,\n",
    "    show_usage_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ToDLwn6cMbg2"
   },
   "outputs": [],
   "source": [
    "# Exemple 3: API CRM\n",
    "ask_agent(\"Qui sont nos meilleurs clients et combien ont ils d√©pens√© ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrfhL7rMMbg3"
   },
   "outputs": [],
   "source": [
    "# Exemple 3: API CRM\n",
    "ask_agent(\n",
    "    question=\"Qui sont nos meilleurs commerciaux et quelles sont leurs performances?\",\n",
    "    verbose=True,\n",
    "    show_tool_details=True,\n",
    "    show_usage_stats=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdP2OjtXMbg3"
   },
   "outputs": [],
   "source": [
    "# Exemple 4: Question complexe n√©cessitant plusieurs sources\n",
    "ask_agent(\"Quels sont nos produits les plus vendus et y a-t-il des opportunit√©s commerciales en cours pour ces mod√®les?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBcq03CnMbg3"
   },
   "outputs": [],
   "source": [
    "# Exemple 4: Question complexe n√©cessitant plusieurs sources\n",
    "ask_agent(\"Quels sont nos produits les plus vendus et y a-t-il des opportunit√©s commerciales en cours pour ces mod√®les?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQQlBiumMbg3"
   },
   "outputs": [],
   "source": [
    "# Exemple 4: Question complexe n√©cessitant plusieurs sources\n",
    "ask_agent(\"Quels sont nos produits les plus vendus et y a-t-il des opportunit√©s commerciales en cours pour ces mod√®les?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoEKRIO11Yxn"
   },
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "#  Streamlit Chatbot V√©loCorp ‚Äì 100 % autonome dans UNE cellule\n",
    "# =======================================================================\n",
    "#\n",
    "#  ‚ö†Ô∏è  Pr√©requis (√† installer une seule fois)\n",
    "#  ------------------------------------------------\n",
    "#  pip install streamlit langchain langgraph azure-search-documents \\\n",
    "#              sentence-transformers pyodbc requests\n",
    "#\n",
    "#  ‚öôÔ∏è  Variables d‚Äôenvironnement attendues\n",
    "#  ------------------------------------------------\n",
    "#  OPENAI_API_KEY          = <cl√© Azure OpenAI>\n",
    "#  OPENAI_ENDPOINT         = <https://...>.openai.azure.com\n",
    "#  OPENAI_DEPLOYMENT       = <nom du d√©ploiement chat>\n",
    "#  OPENAI_API_VERSION      = 2024-02-15-preview\n",
    "#\n",
    "#  SEARCH_ENDPOINT         = https://....search.windows.net\n",
    "#  SEARCH_KEY              = <cl√© Admin ou Query>\n",
    "#\n",
    "#  SQL_SERVER              = <host.database.windows.net>\n",
    "#  SQL_DATABASE            = <nom BDD>\n",
    "#  SQL_USERNAME            = <login SQL>\n",
    "#  SQL_PASSWORD            = <password>\n",
    "#\n",
    "#  API_BASE_URL            = https://api.velocorp.com\n",
    "#\n",
    "#  ‚ûú Adaptez si besoin, ou remplacez les os.getenv(...) ci-dessous par\n",
    "#    des cha√Ænes ‚Äúen dur‚Äù pour un test rapide.\n",
    "# -----------------------------------------------------------------------\n",
    "import streamlit as st\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üìã Configuration (charg√©e depuis les variables d‚Äôenvironnement)\n",
    "# ----------------------------------------------------------------------\n",
    "config: Dict[str, str] = {\n",
    "    \"openai_key\":        os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"openai_endpoint\":   os.getenv(\"OPENAI_ENDPOINT\"),\n",
    "    \"chat_deployment\":   os.getenv(\"OPENAI_DEPLOYMENT\"),\n",
    "    \"openai_version\":    os.getenv(\"OPENAI_API_VERSION\", \"2024-02-15-preview\"),\n",
    "    \"search_endpoint\":   os.getenv(\"SEARCH_ENDPOINT\"),\n",
    "    \"search_key\":        os.getenv(\"SEARCH_KEY\"),\n",
    "    \"sql_server\":        os.getenv(\"SQL_SERVER\"),\n",
    "    \"sql_database\":      os.getenv(\"SQL_DATABASE\"),\n",
    "    \"sql_username\":      os.getenv(\"SQL_USERNAME\"),\n",
    "    \"sql_password\":      os.getenv(\"SQL_PASSWORD\"),\n",
    "    \"api_base_url\":      os.getenv(\"API_BASE_URL\"),\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîß Utilitaires & classes d√©j√† fournis plus haut (inchang√©s)\n",
    "#    ‚¨áÔ∏è  Copi√©s/coll√©s tels quels pour √™tre self-contained\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------- Embeddings cache -------------------------------------------------\n",
    "_EMBED_MODEL = None\n",
    "def _get_embedder(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    global _EMBED_MODEL\n",
    "    if _EMBED_MODEL is None:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        _EMBED_MODEL = SentenceTransformer(model_name)\n",
    "    return _EMBED_MODEL\n",
    "\n",
    "# ---------- Tracker ----------------------------------------------------------\n",
    "class ToolUsageTracker:\n",
    "    def __init__(self): self.reset()\n",
    "    def reset(self):\n",
    "        self.tool_calls = []\n",
    "        self.session_start = datetime.now()\n",
    "    def track_tool_call(self, tool_name: str, args: dict,\n",
    "                        start_time: float, end_time: float,\n",
    "                        success: bool, result_summary: str = \"\", error: str = \"\"):\n",
    "        self.tool_calls.append({\n",
    "            \"tool_name\": tool_name,\n",
    "            \"args\": args,\n",
    "            \"start_time\": datetime.fromtimestamp(start_time),\n",
    "            \"end_time\": datetime.fromtimestamp(end_time),\n",
    "            \"duration_ms\": round((end_time - start_time) * 1000, 2),\n",
    "            \"success\": success,\n",
    "            \"result_summary\": result_summary,\n",
    "            \"error\": error\n",
    "        })\n",
    "    def get_usage_summary(self):\n",
    "        if not self.tool_calls: return \"Aucun outil utilis√©\"\n",
    "        s = {\n",
    "            \"total_calls\": len(self.tool_calls),\n",
    "            \"total_duration_ms\": sum(c[\"duration_ms\"] for c in self.tool_calls),\n",
    "            \"tools_used\": list({c[\"tool_name\"] for c in self.tool_calls}),\n",
    "        }\n",
    "        return s\n",
    "tool_tracker = ToolUsageTracker()\n",
    "\n",
    "def tracked_tool(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start, name = time.time(), func.__name__\n",
    "        try:\n",
    "            res = func(*args, **kwargs)\n",
    "            tool_tracker.track_tool_call(name, {\"args\": args, \"kwargs\": kwargs},\n",
    "                                         start, time.time(), True,\n",
    "                                         str(res)[:120])\n",
    "            return res\n",
    "        except Exception as e:\n",
    "            tool_tracker.track_tool_call(name, {\"args\": args, \"kwargs\": kwargs},\n",
    "                                         start, time.time(), False, error=str(e))\n",
    "            raise\n",
    "    return wrapper\n",
    "\n",
    "# ---------- OUTILS LangChain -------------------------------------------------\n",
    "@tool\n",
    "@tracked_tool\n",
    "def search_documents(query: str, top_results: int = 3) -> str:\n",
    "    \"\"\"Recherche vectorielle dans l‚Äôindex Azure Cognitive Search 'documents'.\n",
    "      Args:\n",
    "          query: la requ√™te texte de l‚Äôutilisateur.\n",
    "          top_results: nombre de r√©sultats max √† retourner.\n",
    "      Returns:\n",
    "          Aper√ßu markdown des r√©sultats ou message d‚Äôerreur.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        client = SearchClient(config[\"search_endpoint\"], \"documents\",\n",
    "                              AzureKeyCredential(config[\"search_key\"]))\n",
    "        vec = _get_embedder().encode(query).tolist()\n",
    "        if VectorizedQuery:\n",
    "            vq = VectorizedQuery(vector=vec, k=top_results, fields=\"embedding\")\n",
    "            results = client.search(search_text=\"*\", vector_queries=[vq])\n",
    "        else:\n",
    "            from azure.search.documents.models import Vector\n",
    "            results = client.search(search_text=\"*\",\n",
    "                                    vector=Vector(value=vec, k=top_results,\n",
    "                                                  fields=\"embedding\"))\n",
    "        out = []\n",
    "        for i, r in enumerate(results, 1):\n",
    "            out.append(f\"**{i}. {r.get('filename','?')}** ‚Äì \" +\n",
    "                       (r.get('content','')[:150].replace('\\n',' ') + '‚Ä¶'))\n",
    "        return \"\\n\".join(out) or \"Aucun r√©sultat.\"\n",
    "    except Exception as e:\n",
    "        return f\"Erreur : {e}\"\n",
    "\n",
    "@tool\n",
    "@tracked_tool\n",
    "def explore_database_schema(table_name: str = None) -> str:\n",
    "    \"\"\"Explore la structure de la base SQL V√©loCorp.\"\"\"\n",
    "    try:\n",
    "        conn_str = (\n",
    "            f\"DRIVER={{ODBC Driver 18 for SQL Server}};\"\n",
    "            f\"SERVER={config['sql_server']};DATABASE={config['sql_database']};\"\n",
    "            f\"UID={config['sql_username']};PWD={config['sql_password']};\"\n",
    "            \"Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    "        )\n",
    "        with pyodbc.connect(conn_str) as conn:\n",
    "            cur = conn.cursor()\n",
    "            if not table_name:\n",
    "                cur.execute(\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES \"\n",
    "                            \"WHERE TABLE_TYPE='BASE TABLE' ORDER BY 1\")\n",
    "                return \"\\n\".join(\"- \" + r[0] for r in cur.fetchall())\n",
    "            cur.execute(\"SELECT COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS \"\n",
    "                        \"WHERE TABLE_NAME=?\", table_name)\n",
    "            cols = cur.fetchall()\n",
    "            return f\"**{table_name}**\\n\" + \"\\n\".join(f\"- {c[0]} ({c[1]})\" for c in cols)\n",
    "    except Exception as e:\n",
    "        return f\"Erreur sch√©ma : {e}\"\n",
    "\n",
    "@tool\n",
    "@tracked_tool\n",
    "def query_database(sql_query: str) -> str:\n",
    "    if not sql_query.strip().upper().startswith(\"SELECT\"):\n",
    "        \"\"\"Ex√©cute une requ√™te SELECT sur la base V√©loCorp et renvoie un tableau markdown.\"\"\"\n",
    "        return \"Seules les requ√™tes SELECT sont autoris√©es.\"\n",
    "    try:\n",
    "        conn_str = (\n",
    "            f\"DRIVER={{ODBC Driver 18 for SQL Server}};\"\n",
    "            f\"SERVER={config['sql_server']};DATABASE={config['sql_database']};\"\n",
    "            f\"UID={config['sql_username']};PWD={config['sql_password']};\"\n",
    "            \"Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    "        )\n",
    "        with pyodbc.connect(conn_str) as conn:\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(sql_query)\n",
    "            cols = [d[0] for d in cur.description]\n",
    "            rows = cur.fetchmany(20)\n",
    "            header = \" | \".join(cols)\n",
    "            sep = \"-\" * len(header)\n",
    "            lines = [\"```\\n\" + header, sep]\n",
    "            for r in rows:\n",
    "                lines.append(\" | \".join(str(x) for x in r))\n",
    "            lines.append(\"```\")\n",
    "            return \"\\n\".join(lines)\n",
    "    except Exception as e:\n",
    "        return f\"Erreur SQL : {e}\"\n",
    "\n",
    "@tool\n",
    "@tracked_tool\n",
    "def query_crm_api(endpoint: str, params: dict = None) -> str:\n",
    "    try:\n",
    "        r = requests.get(f\"{config['api_base_url']}/crm/{endpoint}\",\n",
    "                         params=params or {}, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        return json.dumps(r.json()[:3], indent=2)[:500]\n",
    "    except Exception as e:\n",
    "        return f\"Erreur API : {e}\"\n",
    "\n",
    "tools = [search_documents, explore_database_schema,\n",
    "         query_database, query_crm_api]\n",
    "\n",
    "# ---------- AGENT LangGraph --------------------------------------------------\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint  = config[\"openai_endpoint\"],\n",
    "    api_key         = config[\"openai_key\"],\n",
    "    deployment_name = config[\"chat_deployment\"],\n",
    "    api_version     = config[\"openai_version\"],\n",
    ")\n",
    "\n",
    "llm_tools = llm.bind_tools(tools)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def agent_node(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "Tu es l‚Äôassistant V√©loCorp. Utilise les outils si n√©cessaire.\n",
    "R√©ponds de fa√ßon concise. Si besoin de SQL : commence par explore_database_schema().\n",
    "\"\"\"\n",
    "    msgs = [HumanMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    resp = llm_tools.invoke(msgs)\n",
    "    return {\"messages\": state[\"messages\"] + [resp]}\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    last = state[\"messages\"][-1]\n",
    "    if hasattr(last, \"tool_calls\") and last.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.add_node(\"tools\", ToolNode(tools))\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.add_conditional_edges(\"agent\", should_continue,\n",
    "                            {\"tools\": \"tools\", END: END})\n",
    "graph.add_edge(\"tools\", \"agent\")\n",
    "app_graph = graph.compile()\n",
    "\n",
    "def ask_agent(question: str) -> str:\n",
    "    tool_tracker.reset()\n",
    "    out = app_graph.invoke({\"messages\": [HumanMessage(content=question)]})\n",
    "    return out[\"messages\"][-1].content\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üåê  INTERFACE STREAMLIT (chat) ‚Äì ultra-compact\n",
    "# ----------------------------------------------------------------------\n",
    "st.set_page_config(page_title=\"Chatbot V√©loCorp\", page_icon=\"üö¥‚Äç‚ôÇÔ∏è\",\n",
    "                   layout=\"centered\")\n",
    "st.title(\"ü§ñ Chatbot V√©loCorp\")\n",
    "\n",
    "if \"history\" not in st.session_state:\n",
    "    st.session_state.history = []\n",
    "\n",
    "for role, msg in st.session_state.history:\n",
    "    st.chat_message(role).markdown(msg)\n",
    "\n",
    "prompt = st.chat_input(placeholder=\"Posez votre question‚Ä¶\")\n",
    "if prompt:\n",
    "    st.chat_message(\"user\").markdown(prompt)\n",
    "    with st.spinner(\"R√©flexion‚Ä¶\"):\n",
    "        response = ask_agent(prompt)\n",
    "    st.chat_message(\"assistant\").markdown(response)\n",
    "    st.session_state.history.append((\"user\", prompt))\n",
    "    st.session_state.history.append((\"assistant\", response))\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üöÄ Auto-lancement Streamlit (si la cellule est ex√©cut√©e comme script)\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\" and st._is_running_with_streamlit is False:\n",
    "    # On √©crit ce code dans un fichier temporaire et on lance `streamlit run`\n",
    "    code = open(__file__, \"r\").read() if \"__file__\" in globals() else sys.modules[__name__].__dict__[\"In\"][-1]\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, suffix=\".py\") as f:\n",
    "        f.write(code)\n",
    "        script_path = f.name\n",
    "\n",
    "    def _run():\n",
    "        subprocess.call([\"streamlit\", \"run\", script_path, \"--server.headless\", \"false\"])\n",
    "\n",
    "    threading.Thread(target=_run, daemon=True).start()\n",
    "    time.sleep(3)\n",
    "    webbrowser.open(\"http://localhost:8501\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qnc0jFas48Ck"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
